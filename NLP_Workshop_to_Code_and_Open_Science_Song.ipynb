{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Workshop to Code and Open Science Song",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniellekurtin/CodingAnOpenScienceSong/blob/main/NLP_Workshop_to_Code_and_Open_Science_Song.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS3hWIuJEe9-"
      },
      "source": [
        "#Mini Hack Consortium Workshop 1:\n",
        "##Using topics generated from Natural Language Processing to inspire lyrics for an Open Science Song \n",
        "\n",
        "\n",
        "**Objective**\n",
        "You will collaborate with your group to work through this Google Colab notebook, where you will use Python to help generate lyrics for a song about open science. You will have a mentor with you who is familar with this workshop and is there to help you answer questions. \n",
        "\n",
        "**Part 1:**\n",
        "We will introduce NLP: what it does, how it works, and how we're going to use it today. \n",
        "\n",
        "**Part 2:**\n",
        "We will introduce the text we're using to generate inspiration for the lyrics for an Open Science Song. We are using a compiled body of n=? papers, studies, and editorials on open and reproducible research. These papers can all be found at this link:\n",
        "\n",
        "**Part 3:**\n",
        "We will learn about data cleaning and preprocessing for NLP, such as text tokenization, text normalization, stemming, part-of-speech tagging. \n",
        "\n",
        "**Part 4:**\n",
        "We will use topic modelling to extract topics from our text corpus. \n",
        "\n",
        "**Intermission:** We will take a __ minute break for lunch.\n",
        "\n",
        "**Part 5:**\n",
        "We will break into different groups to code an open science song togeter. Each breakout room will have a mentor who will help you use the topics generated from the NLP to craft some of an open science song.\n",
        "\n",
        "**Part 6:**\n",
        "Something social, if we'd like\n",
        "\n",
        "**References :**\n",
        "The 1-? parts of this workshop are credited to\n",
        "\n",
        "\n",
        "The ?-? parts of this workshop are credited to Maria Balaet: \n",
        "\n",
        "*This event was organized by: Danielle Kurtin, Maria Balaet, Yorguin Mantilla Ramos,  Zhaoying Yu, Roonak Rezvani, Daniel Brady, Violeta Menendez Gonzalez*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmx5CYFcpfkl"
      },
      "source": [
        "If you want to mount this Colab workbook on your drive, run the line of code below and follow the prompts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlK05tz9FQfm",
        "outputId": "0bd37bf6-ca12-412a-d425-b55e9290c923"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aSaCKA_J3RR"
      },
      "source": [
        "## Importing packages\n",
        "\n",
        "Packages are one of the main reasons Python is an incredible lanugage- we can pull from packages that perform certain functions. Today we'll use the NLTK, or Natural Lanugage Toolkit. To learn more about NLTK, please check out their [documentation](https://www.nltk.org/). \n",
        "\n",
        "However, NLTK is not the only NLP package. Another, more advanced one is called spaCy. Feel free to check out their [documentation](https://spacy.io/), too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkXwl0p1G6Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92a95a7-8b84-477a-afb2-ed643786d250"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH1hnT-QKFBI"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is breaks up a string of words into semantically useful units called \"tokens\". This turns words and letters into a mathematical format, sometimes known as a \"bag-of-words\", where each word becomes a token, which in turn, is a data point.\n",
        "\n",
        "Tokenization can come in two forms- sentence or word tokenization. Sentence tokenization splits sentences within a text, usually by identifying stops (or periods, if you're used to American terminology!). Word tokenization often uses spaces between words to seperate words into tokens. However, more advanced tokenisation functions use collocations to identify words that often go together, like \"New\" and \"York\". \n",
        "\n",
        "Here is an example of first performing sentence tokenization on text, then word tokenization. \n",
        "\n",
        "\"Sadhana is a baker. She enjoys making cakes.\"\n",
        "\n",
        "Sentence tokenization:\n",
        "\n",
        "\"Sadhana is a baker\", \"She enjoys making cakes.\"\n",
        "\n",
        "Word tokenization:\n",
        "\n",
        "\"Sadhana\" \"is\" \"a\" \"baker\" \"She\" \"enjoys\" \"making\" \"cakes\"\n",
        "\n",
        "Tokenization is an important first step in NLP, because a lot of the following steps perform operations on individual words to regularize them. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_pNvP0lDAZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d0a702-77a6-451b-8ab4-a615c5a9c33c"
      },
      "source": [
        "#Tokenization -- Paragraphs into sentences;\n",
        "from nltk.tokenize import sent_tokenize \n",
        "  \n",
        "text = \"Hello All. Welcome to medium. This article is about NLP using NLTK.\"\n",
        "print(\"SENTENCE AS TOKENS:\")\n",
        "print(sent_tokenize(text))\n",
        "print(\"No of Sentence Tokens:\",len(sent_tokenize(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE AS TOKENS:\n",
            "['Hello All.', 'Welcome to medium.', 'This article is about NLP using NLTK.']\n",
            "No of Sentence Tokens: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqkXKzNOG_CP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d122bc4-5be3-4d5b-e52c-24be7bbcb623"
      },
      "source": [
        "import nltk.data \n",
        "  \n",
        "german_tokenizer = nltk.data.load('tokenizers/punkt/PY3/german.pickle') \n",
        "  \n",
        "text = 'Wie geht es Ihnen? Mir geht es gut.'\n",
        "german_tokenizer.tokenize(text) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wie geht es Ihnen?', 'Mir geht es gut.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMAa2dymH7f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9a13da-aa9c-4f90-b6d6-42f782779f54"
      },
      "source": [
        "#Tokenization --Text into word tokens;\n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "text = \"Hello All. Welcome to medium. This article is about NLP using NLTK. Subscribe with $4.00. \"\n",
        "print(\"SENTENCE AS TOKENS:\")\n",
        "print(word_tokenize(text))\n",
        "print(\"No of Sentence Tokens:\",len(word_tokenize(text)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE AS TOKENS:\n",
            "['Hello', 'All', '.', 'Welcome', 'to', 'medium', '.', 'This', 'article', 'is', 'about', 'NLP', 'using', 'NLTK', '.', 'Subscribe', 'with', '$', '4.00', '.']\n",
            "No of Sentence Tokens: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdYqZcYCINaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b5c6e8-f233-4a84-b17e-f2d73db21229"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer \n",
        "text = \"Hello All. Welcome to medium. This article is about NLP using NLTK. Subscribe with $4.00. \"\n",
        "tokenizer = TreebankWordTokenizer() \n",
        "tokenizer.tokenize(text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'All.',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'medium.',\n",
              " 'This',\n",
              " 'article',\n",
              " 'is',\n",
              " 'about',\n",
              " 'NLP',\n",
              " 'using',\n",
              " 'NLTK.',\n",
              " 'Subscribe',\n",
              " 'with',\n",
              " '$',\n",
              " '4.00',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yFvaxNeTc-r"
      },
      "source": [
        "## n-grams \n",
        "\n",
        "n-grams are a way of looking at contiguous sequences of words in a sentence, where the \"n\" specifies the number of words at a time. This can help identify repeated phrases and their importance in the corpus- for example, in our corpus, the words \"open\" and \"science\" will likely appear as the 2-gram \"open science\", or the 4-gram \"open and reproducibile science\".\n",
        "\n",
        "Tokens do not usually have any conditions on contiguity, which is why n-grams come in handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_3jEcxcVbA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747cdc3d-9922-40cf-934a-917304269a0f"
      },
      "source": [
        "#Using pure python\n",
        "\n",
        "import re\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    # Convert to lowercases\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace all none alphanumeric characters with spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "    \n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "    \n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [\" \".join(ngram) for ngram in ngrams]\n",
        "\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "print(text)\n",
        "generate_ngrams(text, n=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello everyone welcome',\n",
              " 'everyone welcome to',\n",
              " 'welcome to intro',\n",
              " 'to intro to',\n",
              " 'intro to machine',\n",
              " 'to machine learning',\n",
              " 'machine learning applications',\n",
              " 'learning applications we',\n",
              " 'applications we are',\n",
              " 'we are now',\n",
              " 'are now learning',\n",
              " 'now learning important',\n",
              " 'learning important basics',\n",
              " 'important basics of',\n",
              " 'basics of nlp']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz-Mq1T6YQSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49917e79-08f4-4f61-9c2f-6d491f03e554"
      },
      "source": [
        "#Using NLTK import ngrams\n",
        "\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "output = list(ngrams(tokens, 3))\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 'everyone', 'welcome'), ('everyone', 'welcome', 'to'), ('welcome', 'to', 'intro'), ('to', 'intro', 'to'), ('intro', 'to', 'machine'), ('to', 'machine', 'learning'), ('machine', 'learning', 'applications'), ('learning', 'applications', 'we'), ('applications', 'we', 'are'), ('we', 'are', 'now'), ('are', 'now', 'learning'), ('now', 'learning', 'important'), ('learning', 'important', 'basics'), ('important', 'basics', 'of'), ('basics', 'of', 'nlp')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78c7sg6ZKOdS"
      },
      "source": [
        "## Text normalization and stemming. \n",
        "\n",
        "Text normalization helps turn all letters in a token into either uppercase or lowercase letters. \n",
        "\n",
        "Stemming removes affixes (prefixes and suffixes) from words, and put them in their basic form.  For example, the word \"running\" would be converted to \"run\"+\"ing\". Here are some more examples:\n",
        "\n",
        "\"noncompliant\"-->\"non\"+\"compliant\"\n",
        "\"fixed\" --> \"fix\"+\"ed\"\n",
        "\"transformation\"-->\"transform\"+\"ation\"\n",
        "\n",
        "You can see how this is better done on individual words than on an entire sentence, or entire body of text! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BG909xTFbeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87813812-ff6a-4eeb-cdaf-7e70bb5f3e56"
      },
      "source": [
        "#Text Normalization\n",
        "\n",
        "#Case Conversion\n",
        "text = \"Hello All. Welcome to medium. This article is about NLP using NLTK. Subscribe with $4.00.\"\n",
        "lowert = text.lower()\n",
        "uppert = text.upper()\n",
        "\n",
        "print(\"To Lower Case:\",lowert)\n",
        "print(\"To Upper Case:\",uppert)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To Lower Case: hello all. welcome to medium. this article is about nlp using nltk. subscribe with $4.00.\n",
            "To Upper Case: HELLO ALL. WELCOME TO MEDIUM. THIS ARTICLE IS ABOUT NLP USING NLTK. SUBSCRIBE WITH $4.00.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl3U13TSaAdW",
        "outputId": "13627403-77b2-433d-9065-ca1809226007"
      },
      "source": [
        "#stemming\n",
        "#Porter stemmer is a famous stemming approach\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer()\n",
        "sentence = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"\n",
        "\n",
        "sent = word_tokenize(sentence)\n",
        "print(\"After Word Tokenization:\\n\",sent)\n",
        "print(\"Total No of Word Tokens: \",len(sent))\n",
        "\n",
        "ps_sent = [ps.stem(words_sent) for words_sent in sent]\n",
        "print(ps_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Word Tokenization:\n",
            " ['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', \"n't\", 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', '.', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', ',', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', '.', 'While', 'I', \"'d\", 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciprocity', '.']\n",
            "Total No of Word Tokens:  69\n",
            "['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'peopl', 'ceas', 'pirat', 'file', 'when', 'those', 'same', 'peopl', 'are', \"n't\", 'paid', 'for', 'their', 'particip', 'in', 'veri', 'lucr', 'network', 'scheme', '.', 'ordinari', 'peopl', 'are', 'relentlessli', 'spi', 'on', ',', 'and', 'not', 'compens', 'for', 'inform', 'taken', 'from', 'them', '.', 'while', 'I', \"'d\", 'like', 'to', 'see', 'everyon', 'eventu', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciproc', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JxdoZyaY-iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07756446-ba0f-4735-ac37-863290bced95"
      },
      "source": [
        "#Porter stemmer is a famous stemming approach\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "ps = PorterStemmer() \n",
        " \n",
        "words = [\"hike\", \"hikes\", \"hiked\", \"hiking\", \"hikers\", \"hiker\", \"universal\", \"universe\", \"university\",\"alumnus\", \"alumni\", \"alumnae\"] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hike  :  hike\n",
            "hikes  :  hike\n",
            "hiked  :  hike\n",
            "hiking  :  hike\n",
            "hikers  :  hiker\n",
            "hiker  :  hiker\n",
            "universal  :  univers\n",
            "universe  :  univers\n",
            "university  :  univers\n",
            "alumnus  :  alumnu\n",
            "alumni  :  alumni\n",
            "alumnae  :  alumna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C75v-V4wZqnb"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "This helps turn words, particularily verbs, into their basic form. For example, \"swim\", \"swam\", and \"swum\" would all be converted to \"swim\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2ihkCM3ktot",
        "outputId": "495846a5-820d-4d72-f29b-ed12026083a8"
      },
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "text = \"Live like a leaf. Leave the world as the leaves fall from the tree.\"\n",
        "words = word_tokenize(text)\n",
        "for w in words: \n",
        "    print(w, \" : \", lemma.lemmatize(w)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Live  :  Live\n",
            "like  :  like\n",
            "a  :  a\n",
            "leaf  :  leaf\n",
            ".  :  .\n",
            "Leave  :  Leave\n",
            "the  :  the\n",
            "world  :  world\n",
            "as  :  a\n",
            "the  :  the\n",
            "leaves  :  leaf\n",
            "fall  :  fall\n",
            "from  :  from\n",
            "the  :  the\n",
            "tree  :  tree\n",
            ".  :  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EM6ADdZYbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfec14a9-0a08-49ed-becd-b549843f4a5f"
      },
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "import re\n",
        "   \n",
        "ps = PorterStemmer() \n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "print(text)\n",
        "\n",
        "\n",
        "#Tokenize and stem the words\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "\n",
        "i=0\n",
        "while i<len(tokens):\n",
        "  tokens[i]=ps.stem(tokens[i])\n",
        "  i=i+1\n",
        "\n",
        "#merge all the tokens to form a long text sequence \n",
        "text2 = ' '.join(tokens) \n",
        "\n",
        "print(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n",
            "hello everyon welcom to intro to machin learn applic We are now learn import basic of nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQg-2u17aWQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812d26ef-6117-4e03-80e3-9719bf8af63f"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize \n",
        "import re\n",
        "   \n",
        "ss = SnowballStemmer(\"english\")\n",
        "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
        "print(text)\n",
        "\n",
        "\n",
        "#Tokenize and stem the words\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
        "\n",
        "i=0\n",
        "while i<len(tokens):\n",
        "  tokens[i]=ss.stem(tokens[i])\n",
        "  i=i+1\n",
        "\n",
        "#merge all the tokens to form a long text sequence \n",
        "text2 = ' '.join(tokens) \n",
        "\n",
        "print(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n",
            "hello everyon welcom to intro to machin learn applic we are now learn import basic of nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULVZ6cWyKrrJ"
      },
      "source": [
        "## Stopword removal\n",
        "\n",
        "Removing stopwords helps clear the amount of \"clutter\" in a sentance. It removes words such as to, then, is, about, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQkySHTBldBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ef450c-b1db-4626-ca98-d769a931368f"
      },
      "source": [
        "#Stopwords removal \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "text = \"Hello All. Welcome to medium. This article is about NLP using NLTK.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "word_tokens = word_tokenize(text) \n",
        "  \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  \n",
        "print(word_tokens) \n",
        "print(filtered_sentence) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'All', '.', 'Welcome', 'to', 'medium', '.', 'This', 'article', 'is', 'about', 'NLP', 'using', 'NLTK', '.']\n",
            "['Hello', 'All', '.', 'Welcome', 'medium', '.', 'This', 'article', 'NLP', 'using', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbM16yS5LExg"
      },
      "source": [
        "## Topic extraction \n",
        "\n",
        "So, we've gone through a lot of work just to get our corpus ready for topic modelling. We've tokenized, stemmed, lemminized, and removed the stopwords from our text- now we're ready to extract topics!\n",
        "\n",
        "To do this, we'll use the following methods:\n",
        "\n",
        "1. Implementation of LDA as topic modelling algorithm\n",
        "2. Rating of top 20 sentences per topic\n",
        "3. Interpretation of top opinions to label/name the topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezey5sr0qWyS"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXOifu3hop5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "daf20792-3c08-49cd-86dc-2e78d1e152b1"
      },
      "source": [
        "## Import packages and read in data\n",
        "\n",
        "from time import time\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tmtoolkit\n",
        "import numpy as np\n",
        "import itertools\n",
        "import logging\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data148=pd.read_pickle('data148_prepared_for_gensim')\n",
        "d148=data148.R148_R_3_lemma.to_list()\n",
        "data=d148\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2139f8a227ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtmtoolkit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tmtoolkit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS7IW6oAqu7q"
      },
      "source": [
        "#coherence of topics with no held out set\n",
        "#Construct df\n",
        "\n",
        "# Topics range\n",
        "min_topics = 1\n",
        "max_topics = 5\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "#data = LIST of cleaned text (no symbols, no numbers, not stopwords)\n",
        "#dictionary = ??\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izO5fM5csm4e"
      },
      "source": [
        "\n",
        "classifier_results = {'Topics': [], \"cv_Coherence_avg\": [], \"umass_Coherence_avg\":[], \"cuci_Coherence_avg\":[], \"cnpmi_Coherence_avg\":[]}\n",
        "    \n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XlBzyPDsnsM"
      },
      "source": [
        "for i in topics_range:\n",
        "    np.random.seed(100) #doesn t work and i have no idea why... i put an issue here https://github.com/RaRe-Technologies/gensim/issues/3218\n",
        "    n_topics = i\n",
        "              \n",
        "    t0 = time()\n",
        "\n",
        "    #Model\n",
        "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                            id2word=id2word,\n",
        "                                                                num_topics=n_topics, \n",
        "                                                                random_state=100,\n",
        "                                                                #chunksize=200,\n",
        "                                                                #passes=100,\n",
        "                                                                workers=20,\n",
        "                                                                iterations=150,\n",
        "                                                                minimum_probability=0)\n",
        "                                                                 \n",
        "    lda_model.save('test/'+directory+'lda_model_n_topics_'+str(i))\n",
        "        \n",
        "    np.random.seed(100)\n",
        "\n",
        "    cv_coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    cv_coherence_total = cv_coherence_model_lda.get_coherence()\n",
        "\n",
        "    ## We are using CV coherence due to our personal research experience, but there are other methods to calculate coherence. Have a play with the following three methods, and compare the outcome to those generated using cv coherence\n",
        "    #umass_coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='u_mass')\n",
        "    #umass_coherence_total = umass_coherence_model_lda.get_coherence()\n",
        "            \n",
        "    #cuci_coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_uci')\n",
        "    #cuci_coherence_total = cuci_coherence_model_lda.get_coherence()\n",
        "    #    \n",
        "    #cnpmi_coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_npmi')\n",
        "    #cnpmi_coherence_total = cnpmi_coherence_model_lda.get_coherence()\n",
        "              \n",
        "    classifier_results['Topics'].append(i)\n",
        "    classifier_results['cv_Coherence_avg'].append(cv_coherence_total)\n",
        "    classifier_results['umass_Coherence_avg'].append(umass_coherence_total)\n",
        "    classifier_results['cuci_Coherence_avg'].append(cuci_coherence_total)\n",
        "    classifier_results['cnpmi_Coherence_avg'].append(cnpmi_coherence_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tw3ZmtZs5U7"
      },
      "source": [
        "\n",
        "np.random.seed(100)\n",
        "opinions = data[col]\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(opinions)\n",
        "\n",
        "# Create Corpus\n",
        "texts = opinions\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "#model\n",
        "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                id2word=id2word,\n",
        "                                                num_topics=n_topics, \n",
        "                                                random_state=100,\n",
        "                                                chunksize=200,\n",
        "                                                passes=100,\n",
        "                                                workers=20,\n",
        "                                                iterations=150,\n",
        "                                                minimum_probability=0)\n",
        "    \n",
        "#model_topics = lda_model.show_topics(formatted=True)\n",
        "#print(model_topics)\n",
        "    \n",
        "#get topic distribution probabilities\n",
        "all_topics = lda_model.get_document_topics(corpus, minimum_probability=0.0)\n",
        "doc_topic_dist_proc = gensim.matutils.corpus2csc(all_topics)\n",
        "doc_topic_dist_numpy = doc_topic_dist_proc.T.toarray()\n",
        "#doc_topic_dist = pd.DataFrame(doc_topic_dist_numpy)\n",
        "\n",
        "#get dominant topic\n",
        "topicnames = ['Topic_' + str(i+1) for i in range(0,n_topics,1)]\n",
        "all_topics_df = pd.DataFrame(doc_topic_dist_numpy,columns=topicnames)\n",
        "all_topics_df['dominant_topic_contribution'] = all_topics_df.max(axis = 1) \n",
        "all_topics_df['dominant_topic'] = np.argmax(all_topics_df.values, axis=1)\n",
        "all_topics_df['dominant_topic_name'] = \"Topic \"+(all_topics_df['dominant_topic']+1).astype(str)\n",
        "    \n",
        "#append df \n",
        "all_topics_df_full = all_topics_df.merge(data[['user_id',col_original]],left_index=True,right_index=True)\n",
        "all_topics_df_full.to_csv('all_labelled_opinions_'+col_original+'_gensim_overfitted.csv')\n",
        "sneak_peak = all_topics_df_full[[col_original]]\n",
        "sneak_peak = sneak_peak.sample(frac=1).reset_index(drop=True)\n",
        "sneak_peak.to_csv('quick_data_sneak_peak_'+col_original+'.csv')\n",
        "    \n",
        "#get the top 20 opinions per topic df and output it as csv\n",
        "top_opinions_df = all_topics_df_full.groupby('dominant_topic_name').apply(lambda x: x.nlargest(n_opinions, 'dominant_topic_contribution')).reset_index(drop=True)\n",
        "top_opinions_df.to_csv('labelled_validation_'+col_original+'_gensim_overfitted.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9YAThRNtBUl"
      },
      "source": [
        "\n",
        "#output .csv for validation with TOP opinions, dominant topic name and column to insert validator opinion\n",
        "validation_sheet1 = top_opinions_df[[col_original,'dominant_topic_name']]\n",
        "validation_sheet1['validation']='yes/no'\n",
        "validation_sheet1=validation_sheet1.sample(frac=1).reset_index(drop=True) #this shuffles the order!!\n",
        "validation_sheet1.to_csv('validation_top_opinions_'+col_original+'_gensim_overfitted.csv')\n",
        "\n",
        "#output .csv for validation with RANDOM opinions, dominant topic name, and column to insert validator opinion\n",
        "#get 20 random opinions per topic from all_topic_df_full\n",
        "\n",
        "validation_sheet2 = []# {'opinion':[],'dominant_topic_name':[], 'validation':[]}\n",
        "for topic in all_topics_df_full.dominant_topic_name.unique():\n",
        "    random_20 = all_topics_df_full[all_topics_df_full['dominant_topic_name']==topic]\n",
        "    random_20 = random_20.sample(n=20)\n",
        "    validation_sheet2.append(random_20)\n",
        "    #random_20 = pd.DataFrame(random_20,columns=all_topics_df_full.columns)\n",
        "validation_sheet2 = pd.concat(validation_sheet2, ignore_index=True)\n",
        "    #validation_sheet2 = pd.DataFrame(validation_sheet2,columns=all_topics_df.columns)\n",
        "\n",
        "validation_sheet2 = validation_sheet2[[col_original,'dominant_topic_name']]\n",
        "validation_sheet2['validation']='yes/no'\n",
        "validation_sheet2=validation_sheet2.sample(frac=1).reset_index(drop=True)\n",
        "pd.DataFrame(validation_sheet2).to_csv('validation_random_opinions_'+col_original+'_gensim_overfitted.csv')\n",
        "\n",
        "#print top words per topic (easier to make the validation for top words manual)\n",
        "\n",
        "for idx, topic in lda_model.show_topics(formatted=False, num_words= 20):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwHCsao-tEFM"
      },
      "source": [
        "result = overfitted_coh(data,'test/')\n",
        "pd.DataFrame(result).to_csv('result_test.csv')\n",
        "sns.lineplot(x='Topics', y='cv_Coherence_avg', data=pd.DataFrame(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9w1n1xtHXG"
      },
      "source": [
        "doc_topic_dist = get_topics_gensim(data148,'R148_R_3_lemma','R148_R_3',7,20,20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}