{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtractText.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFiTDG6O0p2O9lofXP6aBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniellekurtin/NLP_MiniHack/blob/main/ExtractText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Iqbi5uIZptKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a919c6-aae5-4f60-c891-2a1e98544fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Your password has been changed\n",
            "Can't sign in? Forgot your password?\n",
            "Enter your email address below and we will send you the reset instructions\n",
            "If the address matches an existing account you will receive an email with instructions to reset your password.\n",
            "Can't sign in? Forgot your username?\n",
            "Enter your email address below and we will send you your username\n",
            "If the address matches an existing account you will receive an email with instructions to retrieve your username\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Original Articles\n",
            "Correction to:\n",
            "Over the last 50 years, we argue that incentives for academic scientists have become increasingly perverse in terms of competition for research funding, development of quantitative metrics to measure performance, and a changing business model for higher education itself. Furthermore, decreased discretionary funding at the federal and state level is creating a hypercompetitive environment between government agencies (e.g., EPA, NIH, CDC), for scientists in these agencies, and for academics seeking funding from all sources—the combination of perverse incentives and decreased funding increases pressures that can lead to unethical behavior. If a critical mass of scientists become untrustworthy, a tipping point is possible in which the scientific enterprise itself becomes inherently corrupt and public trust is lost, risking a new dark age with devastating consequences to humanity. Academia and federal agencies should better support science as a public good, and incentivize altruistic and ethical outcomes, while de-emphasizing output.\n",
            "T\n",
            " of academia have undergone a dramatic change in the last half century. Competition has increased for tenure-track positions, and most U.S. PhD graduates are selecting careers in industry, government, or elsewhere partly because the current supply of PhDs far exceeds available academic positions (Cyranoski \n",
            "et al.\n",
            ", \n",
            "; Stephan, \n",
            "; Aitkenhead, \n",
            "; Ladner \n",
            "et al.\n",
            ", \n",
            "; Dzeng, \n",
            "; Kolata, \n",
            "). Universities are also increasingly “balance<ing> their budgets on the backs of adjuncts” given that part-time or adjunct professor jobs make up 76% of the academic labor force, while getting paid on average $2,700 per class, without benefits or job security (Curtis and Thornton, \n",
            "; U.S. House Committee on Education and the Workforce, \n",
            "). There are other concerns about the culture of modern academia, as reflected by studies showing that the attractiveness of academic research careers decreases over the course of students' PhD program at Tier-1 institutions relative to other careers (Sauermann and Roach, \n",
            "; Schneider \n",
            "et al.\n",
            ", \n",
            "), reflecting the overemphasis on quantitative metrics, competition for limited funding, and difficulties pursuing science as a public good.\n",
            "In this article, we will (1) describe how perverse incentives and hypercompetition are altering academic behavior of researchers and universities, reducing scientific progress and increasing unethical actions, (2) propose a conceptual model that describes how emphasis on quantity versus quality can adversely affect true scientific progress, (3) consider ramifications of this environment on the next generation of Science, Technology, Engineering and Mathematics (STEM) researchers, public perception, and the future of science itself, and finally, (4) offer recommendations that could help our scientific institutions increase productivity and maintain public trust. We hope to begin a conversation among all stakeholders who acknowledge perverse incentives throughout academia, consider changes to increase scientific progress, and uphold “high ethical standards” in the profession (NAE, \n",
            ").\n",
            "When you rely on incentives, you undermine virtues. Then when you discover that you actually need people who want to do the right thing, those people don't exist…\n",
            "—Barry Schwartz, Swarthmore College (Zetter, \n",
            ")\n",
            "Academics are human and readily respond to incentives. The need to achieve tenure has influenced faculty decisions, priorities, and activities since the concept first became popular (Wolverton, \n",
            "). Recently, however, an emphasis on quantitative performance metrics (Van Noorden, \n",
            "), increased competition for static or reduced federal research funding (e.g., NIH, NSF, and EPA), and a steady shift toward operating public universities on a private business model (Plerou, \n",
            "et al.\n",
            ", \n",
            "; Brownlee, \n",
            "; Kasperkevic, \n",
            ") are creating an increasingly perverse academic culture. These changes may be creating problems in academia at both individual and institutional levels (\n",
            ").\n",
            " \n",
            "Incentive\n",
            "Intended effect\n",
            "Actual effect\n",
            "Modified from Regehr (pers. comm., 2015) with permission.\n",
            "The goal of measuring scientific productivity has given rise to quantitative performance metrics, including publication count, citations, combined citation-publication counts (e.g., h-index), journal impact factors (JIF), total research dollars, and total patents. These quantitative metrics now dominate decision-making in faculty hiring, promotion and tenure, awards, and funding (Abbott \n",
            "et al.\n",
            ", \n",
            "; Carpenter \n",
            "et al.\n",
            ", \n",
            "). Because these measures are subject to manipulation, they are doomed to become misleading and even counterproductive, according to \n",
            "Goodhart's Law\n",
            ", which states that “\n",
            "when a measure becomes a target, it ceases to be a good measure\n",
            "” (Elton, \n",
            "; Fischer \n",
            "et al.\n",
            ", \n",
            "; Werner, \n",
            ").\n",
            "Ultimately, the well-intentioned use of quantitative metrics may create inequities and outcomes worse than the systems they replaced. Specifically, if rewards are disproportionally given to individuals manipulating their metrics, problems of the old subjective paradigms (e.g., old-boys' networks) may be tame by comparison. In a 2010 survey, 71% of respondents stated that they feared colleagues can “game” or “cheat” their way into better evaluations at their institutions (Abbott, \n",
            "), demonstrating that scientists are acutely attuned to the possibility of abuses in the current system.\n",
            "Quantitative metrics are scholar centric and reward output, which is not necessarily the same as achieving a goal of socially relevant and impactful research outcomes. Scientific output as measured by cited work has doubled every 9 years since about World War II (Bornmann and Mutz, \n",
            "), producing “busier academics, shorter and less comprehensive papers” (Fischer \n",
            "et al.\n",
            ", \n",
            "), and a change in climate from “publish or perish” to “funding or famine” (Quake, \n",
            "; Tijdink \n",
            "et al.\n",
            ", \n",
            "). Questions have been raised about how sustainable this exponential increase in the knowledge industry is (Price, \n",
            "; Frodeman, \n",
            ") and how much of the growth is illusory and results from manipulation as per \n",
            "Goodhart's Law\n",
            ".\n",
            "Recent exposés have revealed schemes by journals to manipulate impact factors, use of p-hacking by researchers to mine for statistically significant and publishable results, rigging of the peer-review process itself, and overcitation (Falagas and Alexiou, \n",
            "; Labbé, \n",
            "; Zhivotovsky and Krutovsky, \n",
            "; Bartneck and Kokkelmans, \n",
            "; Delgado López-Cózar \n",
            "et al.\n",
            ", \n",
            "; McDermott, \n",
            "; Van Noorden, \n",
            "; Barry, \n",
            "). A fictional character was recently created to demonstrate a “spamming war in the heart of science,” by generation of 102 fake articles and a stellar h-index of 94 on Google Scholar (Labbé, \n",
            "). Blogs describing how to more discretely raise h-index without committing outright fraud are also commonplace (e.g., Dem, \n",
            ").\n",
            "It is instructive to conceptualize the basic problem from a perspective of emphasizing quality-in-research versus quantity-in-research, as well as effects of perverse incentives (\n",
            "). Assuming that the goal of the scientific enterprise is to maximize true scientific progress, a process that overemphasizes quality might require triple or quadruple blinded studies, mandatory replication of results by independent parties, and peer-review of all data and statistics before publication—such a system would minimize mistakes, but would produce very few results due to overcaution (left \n",
            "). At the other extreme, an overemphasis on quantity is also problematic because accepting less scientific rigor in statistics, replication, and quality controls or a less rigorous review process would produce a very high number of articles, but after considering costly setbacks associated with a high error rate, true progress would also be low. A hypothetical optimum productivity lies somewhere in between, and it is possible that our current practices (enforced by peer review) evolved to be near the optimum in an environment with fewer perverse incentives.\n",
            " True scientific productivity vis-à-vis emphasis on research quality/quantity.\n",
            "However, over the long term under a system of perverse incentives, the true productivity curve changes due to increased manipulation and/or unethical behavior (\n",
            "). In a system overemphasizing quality, there is less incentive to cut corners because checks and balances allow problems to be discovered more easily, but in a system emphasizing quantity, productivity can be dramatically reduced by massive numbers of erroneous articles created by carelessness, subtle falsification (i.e., eliminating bad data), and substandard review if not outright fabrication (i.e., dry labbing).\n",
            "While there is virtually no research exploring the impact of perverse incentives on scientific productivity, most in academia would acknowledge a collective shift in our behavior over the years (\n",
            "), emphasizing quantity at the expense of quality. This issue may be especially troubling for attracting and retaining altruistically minded students, particularly women and underrepresented minorities (WURM), in STEM research careers. Because modern scientific careers are perceived as focusing on “the individual scientist and individual achievement” rather than altruistic goals (Thoman \n",
            "et al.\n",
            ", \n",
            "), and WURM students tend to be attracted toward STEM fields for altruistic motives, including serving society and one's community (Diekman \n",
            "et al.\n",
            ", \n",
            ", Thoman \n",
            "et al.\n",
            ", \n",
            "), many leave STEM to seek careers and work that is more in keeping with their values (e.g., Diekman \n",
            "et al.\n",
            ", \n",
            "; Gibbs and Griffin, \n",
            "; Campbell, \n",
            "et al.\n",
            ", \n",
            ").\n",
            "Thus, another danger of overemphasizing output versus outcomes and quantity versus quality is creating a system that is a “perversion of natural selection,” which selectively weeds out ethical and altruistic actors, while selecting for academics who are more comfortable and responsive to perverse incentives from the point of entry. Likewise, if normally ethical actors feel a need to engage in unethical behavior to maintain academic careers (Edwards, \n",
            "), they may become complicit as per Granovetter's well-established Threshold Model of Collective Behavior (1978). At that point, unethical actions have become “embedded in the structures and processes” of a professional culture, and nearly everyone has been “induced to view corruption as permissible” (Ashforth and Anand, \n",
            ").\n",
            "It is also telling that a new genre of articles termed “quit lit” by the Chronicle of Higher Education has emerged (Chronicle Vitae, \n",
            "), in which successful, altruistic, and public-minded professors give perfectly rational reasons for leaving a profession they once loved—such individuals are easily replaced with new hires who are more comfortable with the current climate. Reasons for leaving range from a saturated job market, lack of autonomy, concerns associated with the very structure of academe (CHE, \n",
            "), and “a perverse incentive structure that maintains the status quo, rewards mediocrity, and discourages potential high-impact interdisciplinary work” (Dunn, \n",
            ").\n",
            "While quantitative metrics provide an objective means of evaluating research productivity relative to subjective measures, now that they have become a target, they cease to be useful and may even be counterproductive. A continued overemphasis on quantitative metrics will pressure all but the most ethical scientists, to overemphasize quantity at the expense of quality, create pressures to “cut corners” throughout the system, and select for scientists attracted to perverse incentives.\n",
            "Scientific societies, research institutions, academic journals and individuals have made similar arguments, and some have signed the San Francisco Declaration of Research Assessment (DORA). The DORA recognizes the need for improving “ways in which output of scientific research are evaluated” and calls for challenging research assessment practices, especially the JIF, which are currently in place. Signatories include the American Society for Cell Biology, American Association for the Advancement of Science, Howard Hughes Medical Institute, and Proceedings of The National Academy of Sciences, among 737 organizations and 12,229 individuals as of June 30, 2016. Indeed, publishers of \n",
            "Nature\n",
            ", \n",
            "Science\n",
            ", and other journals have called for downplaying the JIF metric, and the American Society of Microbiology is announcing plans to “purge the conversation of the impact factor” and remove them from all their journals (Callaway, \n",
            "). The argument is not to get rid of metrics, but to reduce their importance in decision-making by institutions and funding agencies, and perhaps invest resources toward creating more meaningful metrics (ACSB, \n",
            "). DORA would be a step in the right direction of halting the “avalanche” of performance metrics dominating research assessment, which are unreliable and have long been hypothesized to threaten the quality of research (Rice, \n",
            "; Macilwain, \n",
            ").\n",
            "We had to get into the top 100. That was a life-or-death matter for Northeastern.—\n",
            "Richard Freeland, Former President of Northeastern University (Kutner, \n",
            ")\n",
            "The perverse incentives for academic institutions are growing in scope and impact, as best exemplified by \n",
            "U.S. News & World Report\n",
            " annual rankings that purportedly measure “academic excellence” (Morse, \n",
            "). The rankings have strongly influenced, positively or negatively, public perceptions regarding the quality of education and opportunities they offer (Casper, \n",
            "; Gladwell, \n",
            "; Tierney, \n",
            "). Although \n",
            "U.S. News & World Report\n",
            " rankings have been dismissed by some, they still undeniably wield extraordinary influence on college administrators and university leadership—the perceptions created by the objective quantitative ranking determines “how students, parents, high schools, and colleges pursue and perceive education” in practice (Kutner, \n",
            "; Segal, \n",
            ").\n",
            "The rankings rely on subjective proprietary formula and algorithms, the original validity of which has since been undermined by \n",
            "Goodhart's law\n",
            "—universities have attempted to game the system by redistributing resources or investing in areas that the ranking metrics emphasize. Northeastern University, for instance, unapologetically rose from #162 in 1996 to #42 in 2015 by explicitly changing their class sizes, acceptance rates, and even peer assessment. Others have cheated by reporting incorrect statistics (Bucknell University, Claremont-McKenna College, Clemson University, George Washington University, and Emory University are examples of those who were caught) to rise in the ranks (Slotnik and Perez-Pena, \n",
            "; Anderson, \n",
            "; Kutner, \n",
            "). More than 90% of 576 college admission officers thought other institutions were submitting false data to \n",
            "U.S. News\n",
            " according to a 2013 Gallup and Inside Higher Ed poll (Jaschik, \n",
            "), which creates further pressures to cheat throughout the system to maintain a ranking perceived to be fair as discussed in preceding sections.\n",
            "If the work you propose to do isn't virtually certain of success, then it won't get funded—\n",
            "Roger Kornberg, Nobel laureate (Lee, \n",
            ")\n",
            "The only people who can survive in this environment are people who are absolutely passionate about what they're doing and have the self-confidence and competitiveness to just go back again and again and just persistently apply for funding—\n",
            "Robert Waterland, Baylor College of Medicine (Harris and Benincasa, \n",
            ")\n",
            "The federal government's role in financing research and development (R&D), creating new knowledge, or fulfilling public missions like national security, agriculture, infrastructure, and environmental health has become paramount. The cost of high-risk, long-term research, which often has uncertain prospects and/or utility, has been largely borne by the U.S. government in the aftermath of World War II, forming part of an ecosystem with universities and industries contributing to the collective progress of mankind (Bornmann and Mutz, \n",
            "; Hourihan, \n",
            ").\n",
            "However, in the current competitive global environment where China is projected to outspend the U.S. on R&D by 2020, some worry that the “edifice of American innovation rests on an increasingly rickety foundation” because of a decline in spending on federal R&D in the past decade (Casassus, \n",
            "; OECD, \n",
            "; MIT, \n",
            "; Porter, \n",
            "). U.S. “Research Intensity” (i.e., federal R&D as a share of the country's gross domestic product or GDP) has declined to 0.78% (2014), which is down from about 2% in the 1960 s (\n",
            "). With discretionary spending of federal budgets projected to decrease, research intensity is likely to drop even further, despite increased industry funding (Hourihan, \n",
            ").\n",
            " Trends in research intensity (i.e., ratio of U.S. R&D to gross domestic product), roles of federal, business, and other nonfederal funding for R&D: 1953–2013. Data source: National Science Foundation, National Center for Science and Engineering Statistics, National Patterns of R&D Resources (annual series). R&D, research and development.\n",
            "A core mission of American colleges and universities has been “service to the public,” and this goal will be more difficult to reach as universities morph into profit centers churning out patents and new products (Faust, \n",
            "; Mirowski, \n",
            "; Brownlee, \n",
            "; Hinkes-Jones, \n",
            "; Seligsohn, \n",
            "; American Academy of Arts and Sciences, \n",
            "). Until the late 2000s, research institutions and universities went on a building spree fueled by borrowing, with an expectation that increased research funding would allow them to further boost research productivity—a cycle that went bust after the 2007–2008 financial crash (Stephan, \n",
            "). Universities are also allowed to offset debt from ill-fated expansion efforts as indirect costs (Stephan, \n",
            "), which increases overhead and decreases dollars available to spend on research even if funds raised by grants remain constant.\n",
            "The static or declining federal investment in research has created the “worst research funding <scenario in 50 years>” and further ratcheted competition for funding (Lee, \n",
            "; Quake, \n",
            "; Harris and Benincasa, \n",
            "; Schneider \n",
            "et al.\n",
            ", \n",
            "; Stein, \n",
            "), given that the number of researchers competing for grants is rising. The funding rate for NIH grants fell from 30.5% to 18% between 1997 and 2014, and the average age for a first time PI on an R01-equivalent grant has increased to 43 years (NIH, \n",
            ", \n",
            "). NSF funding rates have remained stagnant between 23 and 25% in the past decade (NSF, \n",
            "). While these funding rates are still well above the breakeven point of 6%, at which the net cost of proposal writing equals the net value obtained from a grant by the grant winner (Cushman \n",
            "et al.\n",
            ", \n",
            "), there is little doubt the grant environment is hypercompetitive, susceptible to reviewer biases, and strongly dependent on prior success as measured by quantitative metrics (Lawrence, \n",
            "; Fang and Casadevall, \n",
            "). Researchers must tailor their thinking to align with solicited funding, and spend about half of their time addressing administrative and compliance, drawing focus away from scientific discovery and translation (NSB, \n",
            "; Schneider \n",
            "et al.\n",
            ", \n",
            "; Belluz \n",
            "et al.\n",
            ", \n",
            ").\n",
            "Science is a human endeavor, and despite its obvious historical contributions to advancement of civilization, there is growing evidence that today's research publications too frequently suffer from lack of replicability, rely on biased data-sets, apply low or substandard statistical methods, fail to guard against researcher biases, and their findings are overhyped (Fanelli, \n",
            "; Aschwanden, \n",
            "; Belluz and Hoffman, \n",
            "; Nuzzo, \n",
            "; Gobry, \n",
            "; Wilson, \n",
            "). A troubling level of unethical activity, outright faking of peer review and retractions, has been revealed, which likely represents just a small portion of the total, given the high cost of exposing, disclosing, or acknowledging scientific misconduct (Marcus and Oransky, \n",
            "; Retraction Watch, \n",
            "; BBC, \n",
            "; Borman, \n",
            "). Warnings of systemic problems go back to at least 1991, when NSF Director Walter E. Massey noted that the size, complexity, and increased interdisciplinary nature of research in the face of growing competition was making science and engineering “more vulnerable to falsehoods” (The New York Times, \n",
            ").\n",
            "Misconduct is not limited to academic researchers. Federal agencies are also subject to perverse incentives and hypercompetition, giving rise to a new phenomenon of institutional scientific research misconduct (Lewis, \n",
            "; Edwards, \n",
            "). Recent exemplars uncovered by the first author in the Flint and Washington D.C. drinking water crises include “scientifically indefensible” reports by the U.S. Centers for Disease Control and Prevention (U.S. Centers for Disease Control and Prevention, \n",
            "; U.S. House Committee on Science and Technology, \n",
            "), reports based on nonexistent data published by the U.S. EPA and their consultants in industry journals (Reiber and Dufresne, \n",
            "; Boyd \n",
            "et al.\n",
            ", \n",
            "; Edwards, \n",
            "; Retraction Watch, \n",
            "; U.S. Congress House Committee on Oversight and Government Reform, \n",
            "), and silencing of whistleblowers in EPA (Coleman-Adebayo, \n",
            "; Lewis, \n",
            "; U.S. Congress House Committee on Oversight and Government Reform, \n",
            "). This problem is likely to increase as agencies increasingly compete with each other for reduced discretionary funding. It also raises legitimate and disturbing questions as to whether accepting research funding from federal agencies is inherently ethical or not—modern agencies clearly have conflicts similar to those that are accepted and well understood for industry research sponsors. Given the mistaken presumption of research neutrality by federal funding agencies (Oreskes and Conway, \n",
            "), the dangers of institutional research misconduct to society may outweigh those of industry-sponsored research (Edwards, \n",
            ").\n",
            "A “trampling of the scientific ethos” witnessed in areas as diverse as climate science and galvanic corrosion undermines the “credibility of everyone in science” (Bedeian \n",
            "et al.\n",
            ", \n",
            "; Oreskes and Conway, \n",
            "; Edwards, \n",
            "; Leiserowitz \n",
            "et al.\n",
            ", \n",
            "; The Economist, \n",
            "; BBC, \n",
            "). \n",
            "The Economist\n",
            " recently highlighted the prevalence of shoddy and nonreproducible modern scientific research and its high financial cost to society—posing an open question as to whether modern science was trustworthy, while calling upon science to reform itself (The Economist, \n",
            "). And, while there are hopes that some problems could be reduced by practices that include open data, open access, postpublication peer review, metastudies, and efforts to reproduce landmark studies, these can only partly compensate for the high error rates in modern science arising from individual and institutional perverse incentives (\n",
            ").\n",
            "The National Science Foundation defines research misconduct as intentional “fabrication, falsification, or plagiarism in proposing, performing, or reviewing research, or in reporting research results” (Steneck, \n",
            "; Fischer, \n",
            "). Nationally, the percentage of guilty respondents in research misconduct cases investigated by the Department of Health and Human Services (includes NIH) and NSF ranges from 20% to 33% (U.S. Department of Health and Human Services, \n",
            "; Kroll, 2015, pers. comm.). Direct costs of handling each research misconduct case are $525,000, and over $110 million are incurred annually for all such cases at the institutional level in the U.S (Michalek, \n",
            "et al.\n",
            ", \n",
            "). A total of 291 articles retracted due to misconduct during 1992–2012 accounted for $58 M in direct funding from the NIH, which is less than 1% of the agency's budget during this period, but each retracted article accounted for about $400,000 in direct costs (Stern \n",
            "et al.\n",
            ", \n",
            ").\n",
            "Obviously, incidence of undetected misconduct is some multiple of the cases judged as such each year, and the true incidence is difficult to predict. A comprehensive meta-analysis of research misconduct surveys between 1987 and 2008 indicated that 1 in 50 scientists admitted to committing misconduct (fabrication, falsification, and/or modifying data) at least once and 14% knew of colleagues who had done so (Fanelli, \n",
            "). These numbers are likely an underestimate considering the sensitivity of the questions asked, low response rates, and the \n",
            "Muhammad Ali effect\n",
            " (a self-serving bias where people perceive themselves as more honest than their peers) (Allison \n",
            "et al.\n",
            ", \n",
            "). Indeed, delving deeper, up to 34% of researchers self-reported that they have engaged in “questionable research practices,” including “dropping data points on a gut feeling” and “changing the design, methodology, and results of a study in response to pressures from a funding source,” whereas up to 72% of those surveyed knew of colleagues who had done so (Fanelli, \n",
            "). One study included in Fanelli's meta-analysis looked at rates of exposure to misconduct for 2,000 doctoral students and 2,000 faculty from the 99 largest graduate departments of chemistry, civil engineering, microbiology, and sociology, and found between 6 and 8% of both students and faculty had direct knowledge of faculty falsifying data (Swazey \n",
            "et al.\n",
            ", \n",
            ").\n",
            "In life science and biomedical research, the percentage of scientific articles retracted has increased 10-fold since 1975, and 67% were due to misconduct (Fang \n",
            "et al.\n",
            ", \n",
            "). Various hypotheses are proposed for this increase, including “lure of the luxury journal,” “pathological publishing,” prevalent misconduct policies, academic culture, career stage, and perverse incentives (Martinson \n",
            "et al.\n",
            ", \n",
            "; Harding \n",
            "et al.\n",
            ", \n",
            "; Laduke, \n",
            "; Schekman, \n",
            "; Buela-Casal, \n",
            "; Fanelli \n",
            "et al.\n",
            ", \n",
            "; Marcus and Oransky, \n",
            "; Sarewitz, \n",
            "). \n",
            "Nature\n",
            " recently declared that “pretending research misconduct does not happen is no longer an option” (Nature, \n",
            ").\n",
            "Academia and science are expected to be self-policing and self-correcting. However, based on our experiences, we believe there are incentives throughout the system that induce all stakeholders to “pretend misconduct does not happen.” Science has never developed a clear system for reporting, investigating, or dealing with allegations of research misconduct, and those individuals who do attempt to police behavior are likely to be frustrated and suffer severe negative professional repercussions (Macilwain, \n",
            "; Kevles, \n",
            "; Denworth, \n",
            "). Academics largely operate on an unenforceable and unwritten honor system, in relation to what is considered fair in reporting research, grant writing practices, and “selling” research ideas, and there is serious doubt as to whether science as a whole can actually be considered self-correcting (Stroebe \n",
            "et al.\n",
            ", \n",
            "). While there are exceptional cases where individuals have provided a reality check on overhyped research press releases in areas deemed potentially transformative (e.g., Eisen, \n",
            "; New Scientist, \n",
            "), limitations of hot research sectors are more often downplayed or ignored. Because every modern scientific mania also creates a quantitative metric windfall for participants and there are few consequences for those responsible after a science bubble finally pops, the only true check on pathological science and a misallocation of resources is the unwritten honor system (Langmuir \n",
            "et al.\n",
            ", \n",
            ").\n",
            "The modern academic research enterprise, dubbed a “Ponzi Scheme” by \n",
            "The Economist\n",
            ", created the existing perverse incentive system, which would have been almost inconceivable to academics of 30–50 years ago (The Economist, \n",
            "). We believe that this creation is a threat to the future of science, and unless immediate action is taken, we run the risk of “normalization of corruption” (Ashforth and Anand, \n",
            "), creating a corrupt professional culture akin to that recently revealed in professional cycling or in the Atlanta school cheating scandal.\n",
            "To review, for the 7 years Lance Armstrong won the Tour de France (1999–2005), 20 out of 21 podium finishers (including Armstrong) were directly tied to doping through admissions, sanctions, public investigations, or failing blood tests. Entire teams cheated together because of a “win-at-all cost culture” that was created and sustained over time because there was no alternative in sight (U.S. ADA, \n",
            "; Rose and Fisher, \n",
            "; Saraceno, \n",
            "). Numerous warning signs were ignored, and a retrospective analysis indicates that more than half of all Tour de France winners since 1980 had either been tested positive for or confessed to doping (Mulvey, \n",
            "). The resultant “culture of doping” put clean athletes under suspicion (CIRC, \n",
            "; Dimeo, \n",
            ") and ultimately brought worldwide disrepute to the sport.\n",
            "Likewise, the Atlanta Public Schools (APS) scandal provides another example of a perverse incentive system run to its logical conclusion, but in an educational setting. Twelve former APS employees were sent to prison and dozens faced ethics sanctions for falsifying students' results on state-standardized tests. The well-intentioned quantitative test results became high stakes to the APS employees, because the law “trigger[s] serious consequences for students (like grade promotion and graduation); schools (extra resources, reorganization, or closure); districts (potential loss of federal funds), and school employees (bonuses, demotion, poor evaluations, or firing)” (Kamenetz, \n",
            "). The APS employees betrayed their stated mission of creating a “caring culture of trust and collaboration [where] every student will graduate ready for college and career,” and participated in creating the illusion of a “high-performing school district” (APS, \n",
            "). Clearly, perverse incentives can encourage unethical behavior to manipulate quantitative metrics, even in an institution where the sole goal was to educate children.\n",
            "An uncontrolled perverse incentive system can create a climate in which participants feel they must cheat to compete, whether it is academia (individual or institutional level) or professional sports. While procycling was ultimately discredited and its rewards were not properly distributed to ethical participants, in science, the loss of altruistic actors and trust, and risk of direct harm to the public and the planet raise the dangers immeasurably.\n",
            "So I have just one wish for you—the good luck to be somewhere where you are free to maintain the kind of integrity I have described, and where you do not feel forced by a need to maintain your position in the organization, or financial support, or so on, to lose your integrity. May you have that freedom—\n",
            "Richard Feynman, Nobel laureate (Feynman, \n",
            ")\n",
            "The culture of academia has undergone dramatic change in the last few decades—quite a bit of it has been for the better. Problems with diversity, work-life balance, funding, efficient teaching, public outreach, and engagement have been recognized and partly addressed.\n",
            "As stewards of the profession, we should continually consider whether our collective actions will leave our field in a state that is better or worse than when we entered it. While factors such as state and federal funding levels are largely beyond our control, we are not powerless and passive actors. Problems with perverse incentives and hypercompetition could be addressed by the following: \n",
            "(1) The scope of the problem must be better understood, by systematically mining the experiences and perceptions held by academics in STEM fields, through a comprehensive survey of high-achieving graduate students and researchers.\n",
            "(2) The National Science Foundation should commission a panel of economists and social scientists with expertise in perverse incentives, to collect and review input from all levels of academia, including retired National Academy members and distinguished STEM scholars. The panel could also develop a list of “best practices” to guide evaluation of candidates for hiring and promotion, from a long-term perspective of promoting science in the public interest and for the public good, and maintain academia as a desirable career path for altruistic ethical actors.\n",
            "(3) Rather than pretending that the problem of research misconduct does not exist, science and engineering students should receive instruction on these subjects at both the undergraduate and graduate levels. Instruction should include a review of real world pressures, incentives, and stresses that can increase the likelihood of research misconduct.\n",
            "(4) Beyond conventional goals of achieving quantitative metrics, a PhD program should also be viewed as an exercise in building character, with some emphasis on the ideal of practicing science as service to humanity (Huber, \n",
            ").\n",
            "(5) Universities need to reduce perverse incentives and uphold research misconduct policies that discourage unethical behavior.\n",
            "The authors wish to thank PhD Candidate William Rhoads from Virginia Tech and three anonymous reviewers from Environmental Engineering Science for their assistance with the article and valuable suggestions.\n",
            "No competing financial interests exist.\n",
            "Eur\n",
            "Testimony to the U.S. Cong. Committee on Oversight and Government Reform on \n",
            "Examining Federal Administration of the Safe Drinking Water Act in Flint\n",
            ", \n",
            "Michigan\n",
            " Hearing, February 3, 2016. 112th Congress. 2nd session\n",
            "et al\n",
            "J\n",
            "Hearing. 114th Congress. 1st Session\n",
            "Michigan\n",
            "Hearing, February 3, 2016. Hearing. 114th Congress. 2nd Session\n",
            "Correction to:\n",
            "Copyright 2017, Mary Ann Liebert, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "# the following code was heavily inspired by https://matix.io/extract-text-from-webpage-using-beautifulsoup-and-python/ \n",
        "\n",
        "# provide list of URLs\n",
        "url=['https://www.liebertpub.com/doi/10.1089/ees.2016.0223','https://journals.sagepub.com/doi/10.1177/1745691617751884','https://sites.google.com/site/tdtdecodingtoolbox']\n",
        "\n",
        "\n",
        "\n",
        "# this is where I'll try to store them\n",
        "output=[]\n",
        "count=0\n",
        "\n",
        "for x in range(len(url)):\n",
        "\n",
        "  # pull the content from the webpage\n",
        "  res = requests.get(url[x])\n",
        "  html_page = res.content\n",
        "  soup = BeautifulSoup(html_page, 'html.parser')\n",
        "  text = soup.find_all(text=True)\n",
        "  \n",
        "  #Use the below to identify tags\n",
        "  #print([tag.name for tag in soup.find_all()])\n",
        "\n",
        "  # These are all the tags we don't want\n",
        "  blacklist = [\n",
        "      '[document]',\n",
        "      'noscript',\n",
        "      'style',\n",
        "      'header',\n",
        "      'footer',\n",
        "      'html',\n",
        "      'meta',\n",
        "      'head', \n",
        "      'input',\n",
        "      'title',\n",
        "      'script',\n",
        "      # My additions are mostly from here on out\n",
        "      'a',\n",
        "      # 'i', # this one looks good, too\n",
        "      'tr',\n",
        "      # 'p',  #This is what we want, so we'll keep it. I printed the output after commenting out each one, to see what it does, and this seems like the most sensible output. It's not perfect, but I'm happy enough with it.\n",
        "      'span',\n",
        "      'div',\n",
        "      'h3',\n",
        "      'th',\n",
        "      'sup',\n",
        "      'caption',\n",
        "      'strong',\n",
        "      'an',\n",
        "      'figcaption',\n",
        "      'iframe',\n",
        "      'h2',\n",
        "      'li',\n",
        "      'contrib-group',\n",
        "      'date-in-citation',\n",
        "      'div',\n",
        "      'ul',\n",
        "      'label',\n",
        "      'h5',\n",
        "      'img',\n",
        "      'figure',\n",
        "      'link',\n",
        "      'td',\n",
        "      'tbody',\n",
        "      'h1',\n",
        "      'h4',\n",
        "      'form',\n",
        "      'fieldset',\n",
        "      'legend',\n",
        "      'button',\n",
        "      'thead'\n",
        "\n",
        "  ]\n",
        "\n",
        "  count+=1\n",
        "\n",
        " # original method-\n",
        "  # for t in text:\n",
        "  #     if t.parent.name not in blacklist:\n",
        "  #         output += '{} '.format(t)\n",
        "\n",
        "\n",
        "  for t in text:\n",
        "    if t.parent.name not in blacklist:\n",
        "      output.append({'value': '{} '.format(t), 'key': count})\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Using pandas- error is in importing pandas\n",
        "#####################################\n",
        "df = pd.DataFrame(output)\n",
        "df.to_csv('output.csv')\n",
        "\n"
      ]
    }
  ]
}